{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 透過BERT實現假新聞預測\n",
    "### 修改自 https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入需要的library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0213 03:56:04.017554 139715234146048 file_utils.py:38] PyTorch version 1.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料並合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('line_fake_news.xlsx')\n",
    "df1 = pd.read_excel(xls, '工作表1', header=None)\n",
    "df2 = pd.read_excel(xls, '工作表2', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_rumor = df1.drop(columns=[0, 1, 4])\n",
    "not_rumor.columns = ['label', 'content']\n",
    "rumor = df2.drop(columns=[0, 1, 4, 5])\n",
    "rumor.columns = ['label', 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>這篇文章寫的真好！介紹給所有家人，值得推薦給全國中老年人看！（別錯過）“喝水”是長壽的第一要...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>生魚片中毒案例日增日本當局呼籲提防海獸胃線蟲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>根據日本論壇網站「Rocket24」報導，一名日本星巴克的前員工表示，自己絕不是故意要讓星巴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>「EXTRA無糖口香糖」含阿斯巴甜，會致癌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>#兒童節優惠~2017全台遊樂園/博物館/觀光工廠連假優惠#宜蘭蘭陽博物館優惠期間：4/4(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>中央氣象局已發佈「尼莎」颱風海上警報，因應颱風來襲，自來水公司第三區管理處呼籲所有自來水用戶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>真逆齡台灣高齡醫學專家陳亮恭醫師，推翻過去陳舊觀念，在《真逆齡》一書中，告訴你抗老大智慧：)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>海倫清桃前天臉書爆炸性發文，沒多久她和苦守寒窯18年的老公戴發奎（葛格）攜手現身自掀她年齡、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>人家結婚干你屁事？政治學博士：結婚是兩個人的事，修法卻是兩千萬人的事！政治學博士曰：「人家結...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>桃園傳吳郭魚羅湖病毒疫情，高雄嚴陣以待；請各位兄弟姐妹、親朋好友最近少碰吳郭魚；目前病毒無藥...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            content\n",
       "387  NOT_RUMOR  這篇文章寫的真好！介紹給所有家人，值得推薦給全國中老年人看！（別錯過）“喝水”是長壽的第一要...\n",
       "157      RUMOR                             生魚片中毒案例日增日本當局呼籲提防海獸胃線蟲\n",
       "85       RUMOR  根據日本論壇網站「Rocket24」報導，一名日本星巴克的前員工表示，自己絕不是故意要讓星巴...\n",
       "47       RUMOR                              「EXTRA無糖口香糖」含阿斯巴甜，會致癌\n",
       "298  NOT_RUMOR  #兒童節優惠~2017全台遊樂園/博物館/觀光工廠連假優惠#宜蘭蘭陽博物館優惠期間：4/4(...\n",
       "389  NOT_RUMOR  中央氣象局已發佈「尼莎」颱風海上警報，因應颱風來襲，自來水公司第三區管理處呼籲所有自來水用戶...\n",
       "245  NOT_RUMOR  真逆齡台灣高齡醫學專家陳亮恭醫師，推翻過去陳舊觀念，在《真逆齡》一書中，告訴你抗老大智慧：)...\n",
       "301  NOT_RUMOR  海倫清桃前天臉書爆炸性發文，沒多久她和苦守寒窯18年的老公戴發奎（葛格）攜手現身自掀她年齡、...\n",
       "58       RUMOR  人家結婚干你屁事？政治學博士：結婚是兩個人的事，修法卻是兩千萬人的事！政治學博士曰：「人家結...\n",
       "120      RUMOR  桃園傳吳郭魚羅湖病毒疫情，高雄嚴陣以待；請各位兄弟姐妹、親朋好友最近少碰吳郭魚；目前病毒無藥..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([rumor, not_rumor], ignore_index=True).sample(frac=1)\n",
    "all_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>生魚片中毒案例日增日本當局呼籲提防海獸胃線蟲</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>根據日本論壇網站「Rocket24」報導，一名日本星巴克的前員工表示，自己絕不是故意要讓星巴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>「EXTRA無糖口香糖」含阿斯巴甜，會致癌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>中央氣象局已發佈「尼莎」颱風海上警報，因應颱風來襲，自來水公司第三區管理處呼籲所有自來水用戶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>真逆齡台灣高齡醫學專家陳亮恭醫師，推翻過去陳舊觀念，在《真逆齡》一書中，告訴你抗老大智慧：)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>海倫清桃前天臉書爆炸性發文，沒多久她和苦守寒窯18年的老公戴發奎（葛格）攜手現身自掀她年齡、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>人家結婚干你屁事？政治學博士：結婚是兩個人的事，修法卻是兩千萬人的事！政治學博士曰：「人家結...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>桃園傳吳郭魚羅湖病毒疫情，高雄嚴陣以待；請各位兄弟姐妹、親朋好友最近少碰吳郭魚；目前病毒無藥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>RUMOR</td>\n",
       "      <td>夫婦吃隔夜菜中毒，丈夫身亡！這四種「隔夜菜」比毒藥還狠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>NOT_RUMOR</td>\n",
       "      <td>知識就是力量～知識决定一個人的行動，願與大家一起努力愛地球！✌分解多久需要多長時間紙巾-2-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            content\n",
       "157      RUMOR                             生魚片中毒案例日增日本當局呼籲提防海獸胃線蟲\n",
       "85       RUMOR  根據日本論壇網站「Rocket24」報導，一名日本星巴克的前員工表示，自己絕不是故意要讓星巴...\n",
       "47       RUMOR                              「EXTRA無糖口香糖」含阿斯巴甜，會致癌\n",
       "389  NOT_RUMOR  中央氣象局已發佈「尼莎」颱風海上警報，因應颱風來襲，自來水公司第三區管理處呼籲所有自來水用戶...\n",
       "245  NOT_RUMOR  真逆齡台灣高齡醫學專家陳亮恭醫師，推翻過去陳舊觀念，在《真逆齡》一書中，告訴你抗老大智慧：)...\n",
       "301  NOT_RUMOR  海倫清桃前天臉書爆炸性發文，沒多久她和苦守寒窯18年的老公戴發奎（葛格）攜手現身自掀她年齡、...\n",
       "58       RUMOR  人家結婚干你屁事？政治學博士：結婚是兩個人的事，修法卻是兩千萬人的事！政治學博士曰：「人家結...\n",
       "120      RUMOR  桃園傳吳郭魚羅湖病毒疫情，高雄嚴陣以待；請各位兄弟姐妹、親朋好友最近少碰吳郭魚；目前病毒無藥...\n",
       "144      RUMOR                        夫婦吃隔夜菜中毒，丈夫身亡！這四種「隔夜菜」比毒藥還狠\n",
       "376  NOT_RUMOR  知識就是力量～知識决定一個人的行動，願與大家一起努力愛地球！✌分解多久需要多長時間紙巾-2-..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = all_data[all_data['content'].str.len()<512]\n",
    "all_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切成訓練及測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "train_df = all_data.sample(frac=0.7, random_state=200) #random state is a seed value\n",
    "test_df = all_data.drop(train_df.index)\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 pretrained 的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0213 03:56:05.841853 139715234146048 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將文章轉換為訓練需要的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, df, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\":\n",
    "            self.df = train_df\n",
    "        else:\n",
    "            self.df = test_df\n",
    "        self.label_map = {'NOT_RUMOR': 0, 'RUMOR': 1}\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df.content.values[idx]\n",
    "            label = self.df.label.values[idx]\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        else:\n",
    "            text_a = self.df.content.values[idx]\n",
    "            label = self.df.label.values[idx]\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a , dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            # 選出機率最高者\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 印出轉換後結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101,  886, 4500, 2641, 6879, 1305, 3022, 2949, 6880, 8024,  679, 1372,\n",
      "        5543,  775, 1062, 6722, 6752,  733, 1032, 2669, 8024, 1315, 3189, 6629,\n",
      "         122,  943, 3299, 8024, 6917, 5543,  775,  912, 1164, 1555, 2421, 6554,\n",
      "        4289, 2835, 2850, 8108, 1039, 4638, 1962, 2434, 8013, 2641, 6879, 1305,\n",
      "        1062, 1385, 6134, 4850, 8024, 8108, 3299, 8153, 3189, 6629, 5635, 8111,\n",
      "        3299, 8130, 3189, 3632, 8024, 3022,  733, 1378, 1266, 2949, 6880, 1642,\n",
      "        2641, 6879, 1305, 8024,  699, 1762,  122, 2207, 3229, 1058, 6752, 6868,\n",
      "        1059, 2157,  912, 1164, 1555, 2421, 8024, 1086,  886, 4500, 2641, 6879,\n",
      "        1305, 6554, 6525, 1476, 1565,  510, 7934, 1259,  510,  912, 4534, 5023,\n",
      "        2900, 2137, 1555, 1501, 8024, 3680,  816, 2218, 1377,  775, 8108, 1039,\n",
      "        1032, 2669,  511, 2641, 6879, 1305, 1062, 1385, 7674, 2428, 5645,  912,\n",
      "        1164, 1555, 2421, 1394,  868, 8024, 2972, 1139, 2949, 6880, 6752, 6868,\n",
      "        1555, 2421, 1377,  775, 6554, 4289, 1032, 2669, 8024,  684, 1032, 2669,\n",
      "        1501, 7517, 1920, 1914, 3221, 3696, 4707,  676, 7623, 2792, 7444, 8024,\n",
      "        1259, 2886, 1476, 1565,  510, 7934, 1259,  510,  676, 6235, 7613, 1757,\n",
      "         510,  912, 4534, 5023, 8024, 1394, 6243, 7361, 7030, 8135, 5857,  816,\n",
      "        8024, 1546,  671, 4638, 3454,  816, 2218, 3221,  671, 2137, 6206,  886,\n",
      "        4500, 2641, 6879, 1305,  769, 3211, 8024,  699, 1762,  122, 2207, 3229,\n",
      "        1058, 6554, 6525,  511,  102])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = FakeNewsDataset(train_df, 'train', tokenizer=tokenizer)\n",
    "BATCH_SIZE = 2\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "\n",
    "sample_idx = 0\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "print(tokens_tensor)\n",
    "print(segments_tensor)\n",
    "print(label_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GPU fine-tune BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0213 03:56:06.845724 139715234146048 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "I0213 03:56:06.846560 139715234146048 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0213 03:56:07.717832 139715234146048 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0213 03:56:10.327166 139715234146048 modeling_utils.py:543] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.bias', 'classifier.weight']\n",
      "I0213 03:56:10.327733 139715234146048 modeling_utils.py:549] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc: 0.467005076142132\n",
      "[epoch 1] loss: 65.654, acc: 0.843\n",
      "[epoch 2] loss: 44.572, acc: 0.949\n",
      "[epoch 3] loss: 25.085, acc: 0.944\n",
      "[epoch 4] loss: 14.896, acc: 1.000\n",
      "[epoch 5] loss: 4.306, acc: 1.000\n",
      "[epoch 6] loss: 2.142, acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0213 03:57:07.954715 139715234146048 configuration_utils.py:118] Configuration saved in fake_news_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] loss: 1.476, acc: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0213 03:57:08.215232 139715234146048 modeling_utils.py:296] Model weights saved in fake_news_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_LABELS = 2\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=NUM_LABELS)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 7\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' % (epoch + 1, running_loss, acc))\n",
    "model.save_pretrained('fake_news_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拿先前的Pretrained model預測現在的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 56 training data\n",
      "0.7715736040609137 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大\n",
    "testset = FakeNewsDataset(test_df, 'test', tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=4, collate_fn=create_mini_batch)\n",
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train56')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 56 training data')\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 56_2 training data\n",
      "0.7258883248730964 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train56_2')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 56_2 training data')\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 134 training data\n",
      "0.8426395939086294 0.8452380952380952\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train134')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 134 training data')\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 140 training data\n",
      "0.8426395939086294 0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train140')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 140 training data')\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 169 training data\n",
      "0.8375634517766497 0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train169')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 169 training data')\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train by 225 training data\n",
      "0.9390862944162437 0.9404761904761905\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('fake_news_model_train225')\n",
    "clear_output()\n",
    "_, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "_, test_acc = get_predictions(model, testloader, compute_acc=True)\n",
    "print('train by 225 training data')\n",
    "print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
